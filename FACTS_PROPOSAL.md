# Facts System - Ground Truth for Belief Validation

## Conceito

**Fato**: Todo conteÃºdo que o modelo teve acesso e foi salvo com proveniÃªncia rastreÃ¡vel.

### DiferenÃ§a: Facts vs Beliefs

| **Facts** | **Beliefs** |
|-----------|-------------|
| Ground truth observado | InferÃªncias/hipÃ³teses |
| Confidence = 1.0 (ou derivado de fonte) | Confidence variÃ¡vel (0-1) |
| ImutÃ¡vel (apÃ³s criaÃ§Ã£o) | MutÃ¡vel (update-on-use) |
| ProveniÃªncia rastreÃ¡vel (UUID) | Derivado de raciocÃ­nio |
| Recuperado por similaridade (vector store) | Propagado por grafo |

**Exemplo**:
```
Fact: "Donald Trump assumiu presidÃªncia dos EUA em 20/01/2025"
  â†’ source: "user_message_abc123"
  â†’ confidence: 1.0
  â†’ embedding: [0.1, 0.2, ...]

Belief: "TransiÃ§Ãµes presidenciais causam volatilidade polÃ­tica"
  â†’ confidence: 0.75 (inferido)
  â†’ supported_by: [fact_abc123]
```

## Arquitetura

### 1. Fact Model

```python
@dataclass
class Fact:
    """Ground truth fact with provenance"""
    id: str  # UUID
    content: str  # The factual statement
    source_type: str  # "user_message", "document", "api", "web"
    source_id: str  # UUID of the source
    timestamp: datetime
    confidence: float = 1.0  # Can be < 1.0 for uncertain sources
    metadata: Dict = field(default_factory=dict)
    embedding: Optional[List[float]] = None  # Semantic embedding
```

### 2. Vector Store

**Storage**: In-memory vector store (numpy-based) com opÃ§Ã£o de persistÃªncia

```python
class FactStore:
    """Vector store for facts with semantic retrieval"""

    def __init__(self):
        self.facts: Dict[str, Fact] = {}  # id â†’ Fact
        self.embeddings: np.ndarray = None  # (n_facts, embedding_dim)
        self.fact_ids: List[str] = []  # Index â†’ fact_id mapping

    def add_fact(self, content: str, source_type: str, source_id: str) -> Fact:
        """Add fact and generate embedding"""
        ...

    def find_similar(self, query: str, k: int = 5) -> List[Tuple[Fact, float]]:
        """Find k most similar facts (cosine similarity)"""
        ...

    def verify_claim(self, claim: str, threshold: float = 0.8) -> Optional[Fact]:
        """Check if claim matches a known fact"""
        ...
```

### 3. Provenance Chain

```
User Message (UUID: msg_abc123)
  â†“
Fact Extraction (automatic or explicit)
  â†“
Fact (UUID: fact_def456, source: msg_abc123)
  â†“
Vector Store (embedding + metadata)
  â†“
Claim Validation (similarity search)
```

### 4. Integration with Claim Validation

**New Validation Flow**:

```python
async def _validate_claim(self, claim: ValidatedClaim) -> ClaimValidationStep:
    """
    Validate claim against facts first, then beliefs

    1. Check facts: Does this claim match a known fact?
       â†’ If yes: Use fact's confidence (usually 1.0)
       â†’ If similar but different: Flag as potential conflict

    2. If no fact match: Check beliefs (K-NN as before)

    3. Calculate error: claim.estimate vs (fact OR belief)
    """

    # Step 1: Check facts
    matching_fact = self.fact_store.verify_claim(claim.content)
    if matching_fact:
        actual = matching_fact.confidence
        source = f"fact:{matching_fact.id[:8]}"
        margin = 0.05  # Tighter margin for facts
    else:
        # Step 2: Fallback to beliefs (existing K-NN logic)
        belief = await self._get_or_create_belief_for_claim(claim)
        actual = belief.confidence
        source = f"belief:{belief.id[:8]}"
        margin = self._get_margin(belief.id)

    # Step 3: Calculate error
    error = actual - claim.confidence_estimate
    ...
```

## Fact Extraction

### Automatic Extraction from Context

```python
class FactExtractor:
    """Extract facts from user messages and documents"""

    async def extract_from_message(self, message: str, message_id: str) -> List[Fact]:
        """
        Use LLM to extract factual statements from message

        Example:
        User: "Li no jornal que Trump venceu a eleiÃ§Ã£o em novembro de 2024"

        Facts extracted:
        1. "Donald Trump venceu eleiÃ§Ã£o presidencial dos EUA"
           - source: user_message_xyz
           - confidence: 0.8 (secondary source)

        2. "EleiÃ§Ã£o presidencial dos EUA ocorreu em novembro de 2024"
           - source: user_message_xyz
           - confidence: 1.0 (temporal fact)
        """
        ...
```

### Manual Fact Addition

```python
# CLI command
/addfact "Donald Trump Ã© presidente dos EUA desde janeiro 2025"

# API
session.add_fact(
    content="Donald Trump Ã© presidente dos EUA desde janeiro 2025",
    source_type="manual",
    source_id="user_input"
)
```

## Validation Examples

### Example 1: Claim Matches Fact

```
User added fact: "Trump assumiu presidÃªncia em 20/01/2025"

User: "quem Ã© presidente dos EUA?"
LLM Claim: "Donald Trump Ã© presidente dos EUA" (confidence: 0.9)

Validation:
  â†’ Fact found: "Trump assumiu presidÃªncia em 20/01/2025" (similarity: 0.92)
  â†’ Actual confidence: 1.0 (from fact)
  â†’ Error: 1.0 - 0.9 = +0.1
  â†’ Within margin: âœ“ (error < 0.05 for facts? Maybe relax to 0.15)

Result: âœ… Claim validated against fact
```

### Example 2: Claim Conflicts with Fact

```
Fact: "Trump assumiu presidÃªncia em 20/01/2025"

User: "Biden ainda Ã© presidente?"
LLM Claim: "Joe Biden Ã© presidente dos EUA" (confidence: 0.85)

Validation:
  â†’ Fact found: "Trump assumiu presidÃªncia..." (semantic conflict detected)
  â†’ Actual: 0.0 (conflict with fact)
  â†’ Error: 0.0 - 0.85 = -0.85
  â†’ OUTSIDE MARGIN

Result: âŒ Claim conflicts with known fact!

Error Message:
```
âŒ CLAIM CONFLICTS WITH FACT

Your claim: "Joe Biden Ã© presidente dos EUA" (confidence: 0.85)
Known fact: "Trump assumiu presidÃªncia em 20/01/2025"
  Source: user_message (2025-01-20)
  Conflict: These statements contradict

Your claim appears outdated or incorrect based on known facts.
```
```

### Example 3: No Fact, Use Belief

```
No facts about "PostgreSQL performance"

User: "PostgreSQL Ã© rÃ¡pido para consultas complexas?"
LLM Claim: "PostgreSQL tem boa performance para queries complexas" (conf: 0.75)

Validation:
  â†’ No matching fact
  â†’ Fallback to belief K-NN
  â†’ Actual: 0.70 (from similar beliefs)
  â†’ Error: 0.70 - 0.75 = -0.05
  â†’ Within margin: âœ“

Result: âœ… Claim validated against beliefs (no facts available)
```

## CLI Commands

### New Commands

```bash
# List facts
/facts [N]

# Show fact details
/fact <id>

# Add fact manually
/addfact "<content>"

# Search facts
/searchfacts "<query>"

# Import facts from file
/importfacts <path>
```

### Example Session

```
You: /addfact "Trump assumiu presidÃªncia dos EUA em 20/01/2025"
âœ“ Fact added: fact_abc123

You: quem Ã© presidente dos EUA?

ðŸ¤– Assistant (claim-based):
Donald Trump Ã© o presidente dos EUA desde janeiro de 2025.

Claims validated:
  âœ“ "Donald Trump Ã© presidente dos EUA" [0.90 â†’ 1.00 (fact), err: +0.10]
    Source: fact_abc123
```

## Implementation Plan

### Phase 1: Core Fact System
1. Create `Fact` dataclass
2. Implement `FactStore` with in-memory vectors
3. Simple embedding generation (use existing SemanticEstimator)
4. Basic CRUD operations

### Phase 2: Integration with Validation
1. Modify `_validate_claim()` to check facts first
2. Add fact-vs-belief priority logic
3. Update error messages to show fact provenance
4. Tighter margins for fact-based validation

### Phase 3: Fact Extraction
1. LLM-powered fact extraction from messages
2. Automatic fact creation from user statements
3. Confidence scoring for secondary sources
4. Fact deduplication (semantic similarity check)

### Phase 4: CLI & UX
1. `/facts`, `/fact`, `/addfact` commands
2. Fact rendering in CLI (table view)
3. Show fact provenance in claim validation
4. Fact conflict warnings

### Phase 5: Persistence (Optional)
1. Save facts to JSON/SQLite
2. Load facts on session start
3. Export/import fact database

## Benefits

1. **Ground Truth**: Claims validated against observed facts, not just inferences
2. **Provenance**: Every fact traceable to source
3. **Conflict Detection**: Automatically flag claims that contradict facts
4. **Learning**: Facts seed belief graph with high-confidence anchors
5. **Debugging**: Clear separation between "what we know" vs "what we believe"

## Edge Cases

### 1. Temporal Facts
```
Fact (2023): "Biden Ã© presidente dos EUA"
Fact (2025): "Trump Ã© presidente dos EUA"

Solution: Facts have timestamps, most recent wins for temporal queries
```

### 2. Contradictory Facts from Different Sources
```
Source A: "Russia invaded Ukraine in 2022"
Source B: "Russia special operation in Ukraine 2022"

Solution: Store both with source confidence, flag conflict for user review
```

### 3. Fact vs Belief Overlap
```
User provides fact: "APIs can timeout"
System already has belief: "APIs can timeout" (confidence: 0.75)

Solution: Upgrade belief to fact, preserve graph connections
```

## Future Extensions

1. **Fact Sources**: Wikipedia, news APIs, documents
2. **Fact Expiration**: Auto-expire temporal facts
3. **Fact Voting**: Multiple sources increase confidence
4. **Fact Chains**: Facts that depend on other facts
5. **Fact Challenges**: User can challenge/dispute facts

---

This system creates a **two-tier epistemology**:
- **Tier 1 (Facts)**: Observed ground truth
- **Tier 2 (Beliefs)**: Inferences and hypotheses

Claims are validated against Tier 1 first (tight margin), falling back to Tier 2 (K-NN with looser margin).
