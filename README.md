# Justification-Based Belief Tracking System

Um sistema de manutenÃ§Ã£o de crenÃ§as neural-simbÃ³lico que combina rastreamento causal determinÃ­stico com propagaÃ§Ã£o semÃ¢ntica probabilÃ­stica.

## ğŸ¯ Conceito Central

Ao invÃ©s de apenas armazenar crenÃ§as isoladas, o sistema mantÃ©m um **grafo de justificaÃ§Ã£o** onde:
- **Nodes**: Beliefs (statements em linguagem natural) com confianÃ§a probabilÃ­stica
- **Edges**: RelaÃ§Ãµes de justificaÃ§Ã£o (A suporta B, A contradiz C)
- **PropagaÃ§Ã£o**: MudanÃ§as se propagam atravÃ©s do grafo via dois mecanismos:
  1. **Causal** (determinÃ­stico): atravÃ©s de links explÃ­citos de justificaÃ§Ã£o
  2. **SemÃ¢ntica** (probabilÃ­stico): atravÃ©s de similaridade de conteÃºdo

## ğŸ—ï¸ Arquitetura

```
belief_types.py              # Estruturas de dados core
â”œâ”€â”€ Belief                   # Node do grafo com confianÃ§a + links
â”œâ”€â”€ PropagationEvent         # Registro de um update individual
â””â”€â”€ PropagationResult        # Resultado de uma cascata completa

justification_graph.py       # Motor principal
â”œâ”€â”€ add_belief()             # Adiciona belief e descobre justificaÃ§Ãµes
â”œâ”€â”€ link_beliefs()           # Cria relacionamentos explÃ­citos
â”œâ”€â”€ propagate_from()         # Inicia cascata de propagaÃ§Ã£o
â””â”€â”€ explain_confidence()     # Gera justificativa em linguagem natural

propagation_strategies.py    # Algoritmos isolados
â”œâ”€â”€ CausalPropagator         # PropagaÃ§Ã£o determinÃ­stica via grafo
â”œâ”€â”€ SemanticPropagator       # PropagaÃ§Ã£o probabilÃ­stica via similaridade
â”œâ”€â”€ ConflictResolver         # DetecÃ§Ã£o e resoluÃ§Ã£o de contradiÃ§Ãµes
â””â”€â”€ PropagationAnalyzer      # MÃ©tricas de consistÃªncia

test_stripe_scenario.py      # ValidaÃ§Ã£o completa
â””â”€â”€ CenÃ¡rio realista: Stripe API failure
```

## ğŸ”‘ Conceitos-Chave

### 1. Dependency Strength (ForÃ§a de DependÃªncia)

Quando uma belief B Ã© justificada por mÃºltiplas beliefs {A1, A2, A3}, a forÃ§a da dependÃªncia Ã© calculada com:
- **Peso base**: `1/n` onde n = nÃºmero de supporters
- **SaturaÃ§Ã£o logÃ­stica**: previne explosÃ£o quando supporters jÃ¡ sÃ£o muito confiantes
- **PonderaÃ§Ã£o relativa**: confianÃ§as sÃ£o normalizadas entre todos os supporters

```python
dependency = base_weight * (logistic(conf_parent) / sum(logistic(conf_all_parents)))
```

**Por quÃª saturation?** Se uma belief jÃ¡ tem confianÃ§a 0.99, aumentÃ¡-la para 0.995 nÃ£o deveria causar cascata massiva.

### 2. Centrality Dampening (Amortecimento por Centralidade)

Beliefs "hub" (com muitos dependentes) propagam com menos forÃ§a:

```python
dampening = 1 / log2(2 + num_dependents)
```

**RazÃ£o**: Uma belief fundamental que suporta 20 outras nÃ£o deve causar micro-ajustes em todas elas a cada pequena mudanÃ§a.

### 3. PropagaÃ§Ã£o Dual

**Causal (70% do peso)**:
- DeterminÃ­stica atravÃ©s de edges explÃ­citos
- Usa cÃ¡lculo matemÃ¡tico de dependency
- Altamente interpretÃ¡vel (pode traÃ§ar caminho)

**SemÃ¢ntica (30% do peso)**:
- ProbabilÃ­stica via similaridade de conteÃºdo
- Captura relacionamentos implÃ­citos
- Menos interpretÃ¡vel (black-box similarity)

**Merge strategy**:
```python
if belief in causal_updates:
    final = causal[belief] * 0.7
    if belief in semantic_updates:
        final += semantic[belief] * 0.3
else:
    final = semantic[belief] * 0.5  # Semantic sozinho Ã© mais fraco
```

## ğŸš€ Uso BÃ¡sico

### Modo V1.5: Com LLM (Recomendado)

```python
from belief_types import Belief
from llm_agents import detect_relationship, resolve_conflict
import asyncio
import os

# Configure API key
os.environ["GOOGLE_API_KEY"] = "your-gemini-api-key"

async def main():
    # Criar beliefs
    b1 = Belief(
        content="Third-party services are reliable",
        confidence=0.7,
        context="infrastructure"
    )

    lesson = Belief(
        content="Stripe API returned 500 errors",
        confidence=0.9,
        context="incident"
    )

    # Detectar relacionamento automaticamente
    analysis = await detect_relationship(b1, lesson)
    print(f"Relationship: {analysis.relationship}")  # "contradicts"
    print(f"Confidence: {analysis.confidence}")      # 0.75

    # Resolver conflito via LLM
    if analysis.relationship == "contradicts":
        resolution = await resolve_conflict(b1, lesson)
        print(f"Resolution: {resolution.resolved_belief}")
        # "Third-party services are generally reliable, but critical
        #  paths like payments need defensive programming"

asyncio.run(main())
```

### Modo V1.0: Manual

```python
from justification_graph import JustificationGraph

# Criar grafo
graph = JustificationGraph(max_depth=4)

# Adicionar beliefs manualmente
b1 = graph.add_belief(
    content="APIs can fail unexpectedly",
    confidence=0.6,
    context="api_reliability"
)

# Link manual
graph.link_beliefs(lesson.id, b1.id)

# Propagar mudanÃ§as
result = graph.propagate_from(origin_id=lesson.id)
```

## ğŸ“Š Exemplo: Stripe API Failure

Execute o exemplo com LLM (requer API key do Gemini):

```bash
export GOOGLE_API_KEY="your-key"
uv run python example_llm_integration.py
```

Ou use o teste V1.0 (sem LLM):

```bash
uv run python test_estimation.py
```

**Estado inicial**:
```
[0.70] Third-party payment services are reliable
[0.60] APIs can fail unexpectedly
[0.40] Skip defensive programming for established services
```

**Evento**: Stripe retorna erro 500

**LiÃ§Ã£o aprendida**: "Payment APIs can have unexpected downtime" (conf: 0.8)

**PropagaÃ§Ã£o**:
1. LiÃ§Ã£o contradiz "Third-party reliable" â†’ cai para 0.45
2. "Skip defensive" perde suporte â†’ cai para 0.26
3. "APIs can fail" Ã© reforÃ§ada (V1.5 feature)

**Estado final**:
```
[0.80] Payment APIs can have unexpected downtime
[0.45] Third-party payment services are reliable  (â†“)
[0.26] Skip defensive programming                 (â†“â†“)
```

## ğŸ›ï¸ ParÃ¢metros de PropagaÃ§Ã£o

```python
class JustificationGraph:
    max_depth = 4                                    # Profundidade mÃ¡xima
    propagation_budget = {0: 8, 1: 5, 2: 3, 3: 2}   # Updates por nÃ­vel
    min_delta_threshold = 0.05                       # MÃ­nimo para propagar
```

**Budget**: Previne explosÃ£o combinatÃ³ria ao limitar updates por nÃ­vel.

**Threshold adaptativo**: `threshold * (1.2 ** depth)` - mais profundo = mais exigente.

## ğŸ”¬ AnÃ¡lise e Debugging

```python
from propagation_strategies import PropagationAnalyzer

# Verificar consistÃªncia interna
score = PropagationAnalyzer.calculate_belief_consistency(graph.beliefs)
# Retorna [0, 1]: beliefs devem ter confianÃ§a â‰¤ mÃ©dia dos supporters

# Identificar beliefs instÃ¡veis
unstable = PropagationAnalyzer.identify_unstable_beliefs(graph.beliefs)
# Retorna IDs de beliefs com alta confianÃ§a mas suporte fraco
```

## ğŸ¯ Casos de Uso

### 1. Agentes AutÃ´nomos
```python
# ApÃ³s falha em task
task_result = {"error": "JSON malformed", "api": "external"}
lesson = extract_lesson(task_result)
belief = graph.add_belief(lesson, confidence=0.7)
graph.propagate_from(belief.id)
```

### 2. Sistemas de RecomendaÃ§Ã£o
```python
# Aprendizado de preferÃªncias
user_feedback = "I don't like spicy food"
preference = graph.add_belief(user_feedback, confidence=0.8)
# Propaga para beliefs relacionadas sobre restaurantes
```

### 3. DiagnÃ³stico MÃ©dico
```python
# Atualizar hipÃ³teses com novos sintomas
symptom = graph.add_belief("Patient has fever", confidence=0.9)
# Propaga para diagnÃ³sticos possÃ­veis
```

## ğŸš§ LimitaÃ§Ãµes do V1.0-minimal

### 1. PropagaÃ§Ã£o Unidirecional
**LimitaÃ§Ã£o**: PropagaÃ§Ã£o vai apenas de supporters â†’ dependents, nÃ£o o inverso.

**Exemplo problemÃ¡tico**:
```
B1: "APIs fail" (0.6)
  â†“ supports
B2: "Validate responses" (0.7)

# Nova evidÃªncia
B3: "Stripe failed" (0.8) â†’ supports B1

# B1 deveria aumentar, mas nÃ£o aumenta no V1.0
```

**SoluÃ§Ã£o (V1.5)**: PropagaÃ§Ã£o bidirecional com pesos diferentes.

### 2. Embeddings Mock
**LimitaÃ§Ã£o**: Similaridade semÃ¢ntica usa Jaccard (overlap de palavras).

**Problema**: "Validate input" e "Check data" sÃ£o sinÃ´nimos mas tÃªm baixo overlap.

**SoluÃ§Ã£o (V1.5)**: Integrar sentence-transformers ou OpenAI embeddings.

### 3. Conflict Resolution Manual
**LimitaÃ§Ã£o**: ContradiÃ§Ãµes precisam ser marcadas manualmente.

**Exemplo**:
```python
# Manual no V1.0
lesson.contradicts.append(b4.id)
b4.update_confidence(-0.25)
```

**SoluÃ§Ã£o (V1.5)**: LLM detecta contradiÃ§Ãµes automaticamente e gera nuances.

### 4. Sem Aprendizado de Estrutura
**LimitaÃ§Ã£o**: Links de justificaÃ§Ã£o sÃ£o criados manualmente ou por heurÃ­sticas.

**SoluÃ§Ã£o (V1.5)**: LLM julga relacionamentos causais:
```python
async def find_justifications(new_belief):
    candidates = rag_search(new_belief.content)
    for c in candidates:
        rel = await llm_judge("Is A a justification for B?")
        if rel == "supports":
            link(c.id, new_belief.id)
```

## ğŸ›£ï¸ Roadmap

### V1.0-minimal âœ…
- [x] Grafo causal bÃ¡sico
- [x] PropagaÃ§Ã£o determinÃ­stica
- [x] DetecÃ§Ã£o de ciclos
- [x] Teste Stripe funcionando

### V1.5 (LLM Integration) âœ… **CONCLUÃDO**
- [x] Relationship discovery via LLM (PydanticAI + Gemini)
- [x] Conflict resolution automÃ¡tico via LLM
- [x] Structured outputs com Pydantic models
- [x] Batch relationship detection
- [ ] PropagaÃ§Ã£o bidirecional (prÃ³ximo)
- [ ] Embeddings reais via Gemini (prÃ³ximo)

### V2.0 (Escalabilidade) ğŸ¯
- [ ] PersistÃªncia (Neo4j + vector DB)
- [ ] Batch propagation (mÃºltiplas lessons)
- [ ] Dashboard de visualizaÃ§Ã£o (NetworkX + Plotly)
- [ ] API REST para integraÃ§Ã£o

### V2.5 (InteligÃªncia) ğŸ§ 
- [ ] Aprendizado de pesos de edges
- [ ] Meta-beliefs ("confio mais em security beliefs")
- [ ] Temporal decay (beliefs antigas perdem relevÃ¢ncia)
- [ ] Active learning (sistema pede clarificaÃ§Ã£o quando incerto)

## ğŸ“š ConexÃµes com Literatura

| Sistema ClÃ¡ssico | Analogia | InovaÃ§Ã£o |
|------------------|----------|----------|
| **TMS (Doyle, 1979)** | Justification graph | Similaridade semÃ¢ntica vs lÃ³gica propositional |
| **SOAR (Laird, 1987)** | Chunking (lesson â†’ belief) | PropagaÃ§Ã£o probabilÃ­stica |
| **ACT-R (Anderson)** | Activation spreading | ConfianÃ§a como proxy para activation |
| **Bayesian Networks** | Prior/posterior updates | LLM como likelihood function nÃ£o-paramÃ©trica |

**ContribuiÃ§Ã£o principal**: Semantizar a propagaÃ§Ã£o - usar proximidade em embedding space como funÃ§Ã£o de influÃªncia ao invÃ©s de regras lÃ³gicas explÃ­citas.

## ğŸ¤ Contribuindo

Ãreas prioritÃ¡rias para contribuiÃ§Ã£o:
1. **Embeddings reais**: Integrar sentence-transformers
2. **LLM integration**: Relationship detection + conflict resolution
3. **VisualizaÃ§Ã£o**: Dashboard interativo
4. **Benchmarks**: Datasets de agent failures

## ğŸ“„ LicenÃ§a

MIT License - use livremente em projetos comerciais ou acadÃªmicos.

## ğŸ™ Agradecimentos

Inspirado por discussÃµes sobre sistemas de manutenÃ§Ã£o de crenÃ§as, Bayesian program learning, e arquiteturas de agentes autÃ´nomos.

---

**Status**: V1.0-minimal completo âœ…
**PrÃ³ximo**: V1.5 (embeddings reais + LLM integration)
**Autor**: Franklin Baldo ([@franklinbaldo](https://github.com/franklinbaldo))
